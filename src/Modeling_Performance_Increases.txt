
# import pystan
import stanity
n_jobs = 4
import pandas as pd
import seaborn as sns
sns.set_color_codes()
%pylab inline
from scipy.stats import norm, truncnorm

data_directory = '../data/'

from pystan.misc import _summary, _array_to_table
def _print_stanfit(fit, pars=None, probs=(0.025, 0.25, 0.5, 0.75, 0.975), digits_summary=2):
        if fit.mode == 1:
            return "Stan model '{}' is of mode 'test_grad';\n"\
                   "sampling is not conducted.".format(fit.model_name)
        elif fit.mode == 2:
            return "Stan model '{}' does not contain samples.".format(fit.model_name)
        if pars is None:
            pars = fit.sim['pars_oi']
            fnames = fit.sim['fnames_oi']

        n_kept = [s - w for s, w in zip(fit.sim['n_save'], fit.sim['warmup2'])]
        header = ""#Inference for Stan model: {}.\n".format(fit.model_name)
        header += "{} chains, each with iter={}; warmup={}; thin={}; \n"
        header = header.format(fit.sim['chains'], fit.sim['iter'], fit.sim['warmup'],
                               fit.sim['thin'], sum(n_kept))
        header += "post-warmup draws per chain={}, total post-warmup draws={}.\n\n"
        header = header.format(n_kept[0], sum(n_kept))
        footer = "\n\nSamples were drawn using {} at {}.\n"\
#             "For each parameter, n_eff is a crude measure of effective sample size,\n"\
#             "and Rhat is the potential scale reduction factor on split chains (at \n"\
#             "convergence, Rhat=1)."
        sampler = fit.sim['samples'][0]['args']['sampler_t']
        date = fit.date.strftime('%c')  # %c is locale's representation
        footer = footer.format(sampler, date)
        s = _summary(fit, pars, probs)
        body = _array_to_table(s['summary'], s['summary_rownames'],
                               s['summary_colnames'], digits_summary)
        return header + body + footer
    
def plot_time_series_inference(model_fit, var='Y_sim', x=None,
                               ax=None, ind=0, **kwargs):
    from scipy.stats import scoreatpercentile
    ci_thresholds = [2.5, 25, 75, 97.5]
    if len(model_fit[var].shape)<3:
        data = model_fit[var]
    else:
        data = model_fit[var][:,ind,:]
    CIs = scoreatpercentile(data, ci_thresholds, axis=0)
    CIs = pd.DataFrame(data=CIs.T, columns=ci_thresholds)
    if ax is None:
        ax=gca()
    if x is None:
        x = arange(data.shape[1])
    ax.fill_between(x, CIs[2.5], CIs[97.5],alpha=.5, **kwargs)
    ax.fill_between(x, CIs[25], CIs[75], **kwargs)
    
    

model_code = {}

model_code['improvement~N(mu,sigma)'] = """
functions{
    int first_observation_ind(vector my_array){
        int t;
        t = 1;
        while(my_array[t] < -900){
          t = t+1;
        }
        return t;
    }
    
    int last_observation_ind(vector my_array, int length){
        int last_observation;
        last_observation = 0; 
        for(t in 1:length){
          if(my_array[t] > -900){
              last_observation = t;
          }
        }
        return last_observation;
    }
    
    
    int count_n_observations(vector my_array) {
        int count;
        count = 0;
        for (t in 1:num_elements(my_array)) {
            if(my_array[t] > -900){
                count = count + 1;
            }
        }
        return count;
    }
    
    real lower_truncated_normal_lpdf(real x, real mu, real sigma, real A) {
        return(normal_lpdf(x | mu, sigma) - normal_lccdf(A | mu, sigma));
    }
}

data {
    int N_time_periods; // number of time periods
    vector[N_time_periods] Y; // value each time period
    
    int N_time_periods_for_inference;
    
    real mu_prior_location;
    real mu_prior_scale;
    
    real sigma_prior_location;
    real sigma_prior_scale;
 
}

transformed data {
  int first_observation;
  int last_observation;
  int N_observed_time_periods_for_inference;
  int r_offset;
  int n_observations;

  first_observation = first_observation_ind(Y[1:N_time_periods_for_inference]);
  last_observation = last_observation_ind(Y[1:N_time_periods_for_inference], 
                      N_time_periods_for_inference);
                      
  N_observed_time_periods_for_inference = last_observation-first_observation + 1;
  r_offset = first_observation-1;
  
  n_observations = count_n_observations(Y);

}
  
parameters {
    real<lower = 0> mu;
    real<lower = 0> sigma;

    vector<lower = 0,upper = 1>[N_observed_time_periods_for_inference-1] r_raw; // updates
}

transformed parameters {
  // Identify where the first and last non-missing data points are in Y
  vector<lower = 0>[N_observed_time_periods_for_inference-1] r; // updates
  
  {
  // Dictate that the total change between each pair of observations is equal to the observed change between them
  // This is relevant for time periods with missing data
  int most_recent_observation;
  most_recent_observation = first_observation;
  for(t in first_observation+1:last_observation) {
      if(Y[t] > -900) {
        r[(most_recent_observation-r_offset):((t-1)-r_offset)] = 
        r_raw[(most_recent_observation-r_offset):((t-1)-r_offset)] /
        sum(r_raw[(most_recent_observation-r_offset):((t-1)-r_offset)]) * 
        (Y[t]-Y[most_recent_observation]);
        most_recent_observation = t;
        }
    }
    }
}

model {
    mu ~ normal(mu_prior_location, mu_prior_scale);
    sigma ~ cauchy(sigma_prior_location, sigma_prior_scale);

    for(t in 1:N_observed_time_periods_for_inference-1){
        target += lower_truncated_normal_lpdf(r[t] | mu, sigma, 0);
    }
}

generated quantities {
    vector[N_time_periods] Y_sim;
    vector[N_time_periods] log_likelihood;
    real mean_change;
    real variance_change;
    
    mean_change = mean(r);
    variance_change = variance(r);
    
    //Fill out data in the missing periods
    for(t in first_observation:last_observation) {
      if(Y[t] > -900){
          Y_sim[t] = Y[t];
      } else{
          Y_sim[t] = Y_sim[t-1] + r[(t-1)-r_offset];
      } 
    }
    {
    real increase_size;
    //Fill out future data points
    for(t in last_observation+1:N_time_periods){
        // Stan cannot yet generate numbers from a truncated distribution directly, so we have to do this silly thing. 
        // As of version 2.12.0, the devs are still talking about it: https://github.com/stan-dev/math/issues/214
        increase_size = -1.0;  
        while (increase_size<0){
            increase_size = normal_rng(mu,sigma);
        }
        Y_sim[t] = increase_size + Y_sim[t-1];
    }
    }
    
    //Fill out past data points
    {
    int t;
    real increase_size;
    t = first_observation;
    while(t>1){
        increase_size = -1.0;  
        while (increase_size<0){
            increase_size = normal_rng(mu,sigma);
        }
        Y_sim[t-1] = Y_sim[t] - increase_size;
        t = t-1;
    }
    }
    
    for(t in 2:N_time_periods){
        if(Y[t] > -900){
                log_likelihood[t] = lower_truncated_normal_lpdf(Y[t]-Y_sim[t-1]| mu, sigma, 0);
        }
    }
}
"""


model_code['improvement~p(theta)N(mu,sigma)'] = """
functions{
    int first_observation_ind(vector my_array){
        int t;
        t = 1;
        while(my_array[t] < -900){
          t = t+1;
        }
        return t;
    }
    
    int last_observation_ind(vector my_array, int length){
        int last_observation;
        last_observation = 0; 
        for(t in 1:length){
          if(my_array[t] > -900){
              last_observation = t;
          }
        }
        return last_observation;
    }
    
    
    int count_n_observations(vector my_array) {
        int count;
        count = 0;
        for (t in 1:num_elements(my_array)) {
            if(my_array[t] > -900){
                count = count + 1;
            }
        }
        return count;
    }
    
    real lower_truncated_normal_lpdf(real x, real mu, real sigma, real A) {
        return(normal_lpdf(x | mu, sigma) - normal_lccdf(A | mu, sigma));
    }
}

data {
    int N_time_periods; // number of time periods
    vector[N_time_periods] Y; // value each time period
    
    int N_time_periods_for_inference;
    
    real mu_prior_location;
    real mu_prior_scale;
    
    real sigma_prior_location;
    real sigma_prior_scale;
    
    real theta_prior_location;
    real theta_prior_scale;
}

transformed data {
  int first_observation;
  int last_observation;
  int N_observed_time_periods_for_inference;
  int r_offset;
  int n_observations;

  first_observation = first_observation_ind(Y[1:N_time_periods_for_inference]);
  last_observation = last_observation_ind(Y[1:N_time_periods_for_inference], 
                      N_time_periods_for_inference);
                      
  N_observed_time_periods_for_inference = last_observation-first_observation + 1;
  r_offset = first_observation-1;
  
  n_observations = count_n_observations(Y);

}
  
parameters {
    real<lower = 0> mu;
    real<lower = 0> sigma;
    real<lower = 0, upper = 1> theta;

    vector<lower = 0,upper = 1>[N_observed_time_periods_for_inference-1] r_raw; // updates
}

transformed parameters {
  // Identify where the first and last non-missing data points are in Y
  vector<lower = 0>[N_observed_time_periods_for_inference-1] r; // updates
  
  {
  // Dictate that the total change between each pair of observations is equal to the observed change between them
  // This is relevant for time periods with missing data
  int most_recent_observation;
  most_recent_observation = first_observation;
  for(t in first_observation+1:last_observation) {
      if(Y[t] > -900) {
        r[(most_recent_observation-r_offset):((t-1)-r_offset)] = 
        r_raw[(most_recent_observation-r_offset):((t-1)-r_offset)] /
        sum(r_raw[(most_recent_observation-r_offset):((t-1)-r_offset)]) * 
        (Y[t]-Y[most_recent_observation]);
        most_recent_observation = t;
        }
    }
    }
}

model {
    mu ~ normal(mu_prior_location, mu_prior_scale);
    sigma ~ cauchy(sigma_prior_location, sigma_prior_scale);
    theta ~ normal(theta_prior_location, theta_prior_scale);

    for(t in 1:N_observed_time_periods_for_inference-1){
        target += log_mix(theta, lower_truncated_normal_lpdf(r[t] | mu, sigma, 0), 
                      lower_truncated_normal_lpdf(r[t] | 0, 0.01, 0));
    }
}

generated quantities {
    vector[N_time_periods] Y_sim;
    vector[N_time_periods] log_likelihood;
    real mean_change;
    real variance_change;
    
    mean_change = mean(r);
    variance_change = variance(r);
    //Fill out data in the missing periods
    for(t in first_observation:last_observation) {
      if(Y[t] > -900){
          Y_sim[t] = Y[t];
      } else{
          Y_sim[t] = Y_sim[t-1] + r[(t-1)-r_offset];
      } 
    }
    {
    real increase_size;
    //Fill out future data points
    for(t in last_observation+1:N_time_periods){
        // Stan cannot yet generate numbers from a truncated distribution directly, so we have to do this silly thing. 
        // As of version 2.12.0, the devs are still talking about it: https://github.com/stan-dev/math/issues/214
        increase_size = -1.0;  
        while (increase_size<0){
            increase_size = normal_rng(mu,sigma);
        }
        Y_sim[t] = bernoulli_rng(theta) * increase_size + Y_sim[t-1];
    }
    }
    
    //Fill out past data points
    {
    int t;
    real increase_size;
    t = first_observation;
    while(t>1){
        increase_size = -1.0;  
        while (increase_size<0){
            increase_size = normal_rng(mu,sigma);
        }
        Y_sim[t-1] = Y_sim[t] - bernoulli_rng(theta) * increase_size;
        t = t-1;
    }
    }
    
    for(t in 2:N_time_periods){
        if(Y[t] > -900){
            if((Y[t]-Y_sim[t-1])==0) {
                log_likelihood[t] = log(1-theta);
            } else {
                log_likelihood[t] = log(theta) + lower_truncated_normal_lpdf(Y[t]-Y_sim[t-1]| mu, sigma, 0);
            }
        }
    }
}
"""

### Random walk, missing data, positive steps
p_missing = 0.3
mu = 5
sigma = 1
n = 100


a = -mu / sigma
data = truncnorm(a, inf, loc=mu,scale=sigma).rvs(n)

time_series = cumsum(data)
missing = rand(n)<p_missing
missing[0] = False
missing[-1] = False
time_series[missing]=nan


hist(data, bins=50)
figure()
plot(time_series)

n_past_steps = 25
n_future_steps = 25
stan_data = {'N_time_periods': len(time_series)+n_future_steps+n_past_steps,
            'N_time_periods_for_inference': len(time_series)+n_past_steps,
             'Y': pd.Series(concatenate((empty(n_past_steps)*nan,
                                         time_series,
                                         empty(n_future_steps)*nan),0)).fillna(-999),
            'mu_prior_location': 3,
            'mu_prior_scale': 1,
            'sigma_prior_location': 1,
            'sigma_prior_scale': 2}

model_fit = stanity.fit(model_code['improvement~N(mu,sigma)'], data=stan_data, n_jobs=n_jobs)

print(_print_stanfit(model_fit, ['mu', 'sigma']))
print("mu: %.2f, inferred mu: %.2f"%(mu, model_fit['mu'].mean()))
print("sigma: %.2f, inferred sigma: %.2f"%(sigma, model_fit['sigma'].mean()))

figure()
plot_time_series_inference(model_fit)
scatter(arange(len(time_series))+n_past_steps, time_series,s=2)
xlim(xmin=0)
# ylim(ymin=0)

### Random walk, missing data, positive steps, possibility of no step, noise on no step
p_missing = 0.3
theta = .8
mu = 5
sigma = 1
n = 100

improvement = rand(n)<theta
improvement[0] = True

a = -mu / sigma
data = truncnorm(a, inf, loc=mu,scale=sigma).rvs(n)
data[~improvement]=0

time_series = cumsum(data)
missing = rand(n)<p_missing
missing[0] = False
missing[-1] = False
time_series[missing]=nan


hist(data, bins=50)
figure()
plot(time_series)



n_past_steps = 25
n_future_steps = 25
stan_data = {'N_time_periods': len(time_series)+n_future_steps+n_past_steps,
            'N_time_periods_for_inference': len(time_series)+n_past_steps,
             'Y': pd.Series(concatenate((empty(n_past_steps)*nan,
                                         time_series,
                                         empty(n_future_steps)*nan),0)).fillna(-999),
           'theta_prior_location': .8,
           'theta_prior_scale': 2,
            'mu_prior_location': 3,
            'mu_prior_scale': 1,
            'sigma_prior_location': 1,
            'sigma_prior_scale': 2}

model_fit = stanity.fit(model_code['improvement~p(theta)N(mu,sigma)'], data=stan_data, n_jobs=n_jobs)

print(_print_stanfit(model_fit, ['mu', 'sigma','theta']))

print("mu: %.2f, inferred mu: %.2f"%(mu, model_fit['mu'].mean()))
print("sigma: %.2f, inferred sigma: %.2f"%(sigma, model_fit['sigma'].mean()))
print("theta: %.2f, inferred theta: %.2f"%(theta, model_fit['theta'].mean()))

figure()
plot_time_series_inference(model_fit)
scatter(arange(len(time_series))+n_past_steps, time_series,s=2)
xlim(xmin=0)
# ylim(ymin=0)

data = pd.read_csv(data_directory+'time_series.csv',index_col=0)
data = data.reindex(arange(data.index[0],data.index[-1]+1))
metadata = pd.read_csv(data_directory+'time_series_metadata.csv')
technology_models = {}
technology_models_prediction = {}

target_tech_names = metadata.ix[metadata['Type']=='Performance', 'Name']
print("%i technologies"%target_tech_names.shape[0])

technology_models['improvement~p(theta)N(mu,sigma)'] = pd.DataFrame(columns=['mu', 'sigma', 
                                                                             'theta', 
                                                                             'mean', 'variance',
                                                                             'mean_analytic', 'variance_analytic',
                                                                            'log_likelihood'], 
                                                                    index=target_tech_names)
technology_models['improvement~N(mu,sigma)'] = pd.DataFrame(columns=['mu', 'sigma', 
                                                                     'mean', 'variance',
                                                                    'mean_analytic', 'variance_analytic',
                                                                    'log_likelihood'], 
                                                            index=target_tech_names)

n_future_steps = 0
for tech_name in target_tech_names:
    print('------------------------------------')
    print(tech_name)
    print('------------------------------------')
    figure()
    title(tech_name)
    
    time_series = log10(data[tech_name]**-1)
    scatter(arange(len(time_series)), time_series,s=2)

    stan_data = {'N_time_periods': len(time_series),
            'N_time_periods_for_inference': len(time_series),
             'Y': pd.Series(concatenate((time_series,
                                         empty(n_future_steps)*nan),0)).fillna(-999),
            'mu_prior_location': .1,
            'mu_prior_scale': 2,
            'sigma_prior_location': 1,
            'sigma_prior_scale': 2}
    
    ###
    model_fit = stanity.fit(model_code['improvement~N(mu,sigma)'], data=stan_data, n_jobs=n_jobs)
    print(_print_stanfit(model_fit, ['mu', 'sigma']))
    technology_models['improvement~N(mu,sigma)'].ix[tech_name,'mu'] = model_fit['mu'].mean()
    technology_models['improvement~N(mu,sigma)'].ix[tech_name,'sigma'] = model_fit['sigma'].mean()
    technology_models['improvement~N(mu,sigma)'].ix[tech_name,'mean'] = model_fit['mean_change'].mean()
    technology_models['improvement~N(mu,sigma)'].ix[tech_name,'variance'] = model_fit['variance_change'].mean()
    technology_models['improvement~N(mu,sigma)'].ix[tech_name, 'mean_analytic'] = truncnorm(0, 10000, 
                                                         loc=model_fit['mu'].mean(), 
                                                         scale=model_fit['sigma'].mean()).mean()
    technology_models['improvement~N(mu,sigma)'].ix[tech_name, 'variance_analytic'] = truncnorm(0, 10000, 
                                                         loc=model_fit['mu'].mean(), 
                                                         scale=model_fit['sigma'].mean()).var()
    technology_models['improvement~N(mu,sigma)'].ix[tech_name,'log_likelihood'] = nanmean(model_fit['log_likelihood'])

    plot_time_series_inference(model_fit, color='g')
    
    ###
    stan_data['theta_prior_location'] = .5
    stan_data['theta_prior_scale'] = 2
    
    model_fit = stanity.fit(model_code['improvement~p(theta)N(mu,sigma)'], data=stan_data, n_jobs=n_jobs)
    print(_print_stanfit(model_fit, ['mu', 'sigma','theta']))
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name,'mu'] = model_fit['mu'].mean()
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name,'sigma'] = model_fit['sigma'].mean()
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name,'theta'] = model_fit['theta'].mean()
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name,'mean'] = model_fit['mean_change'].mean()
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name,'variance'] = model_fit['variance_change'].mean()
    
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name, 'mean_analytic'] = model_fit['theta'].mean() * truncnorm(0, 10000, 
                                                         loc=model_fit['mu'].mean(), 
                                                         scale=model_fit['sigma'].mean()).mean()
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name, 'variance_analytic'] = model_fit['theta'].mean() * truncnorm(0, 10000, 
                                                         loc=model_fit['mu'].mean(), 
                                                         scale=model_fit['sigma'].mean()).var()
    technology_models['improvement~p(theta)N(mu,sigma)'].ix[tech_name,'log_likelihood'] = nanmean(model_fit['log_likelihood'])
        
    plot_time_series_inference(model_fit, color='b')

technology_models['improvement~N(mu,sigma)'].plot('mean', 'variance', kind='scatter')
figure()
technology_models['improvement~p(theta)N(mu,sigma)'].plot('mean', 'variance', kind='scatter')

technology_models['improvement~N(mu,sigma)'].plot('mean_analytic', 'variance_analytic', kind='scatter')

technology_models['improvement~N(mu,sigma)'].plot('mean_analytic', 'mean', kind='scatter')
figure()
technology_models['improvement~N(mu,sigma)'].plot('variance_analytic', 'variance', kind='scatter')

target_tech_names = metadata.ix[metadata['Type']=='Performance', 'Name']
print("%i technologies"%target_tech_names.shape[0])

training_years = arange(1950,2000,10)
horizons = [5,10,'all']

technology_models_prediction['improvement~p(theta)N(mu,sigma)'] = pd.Panel(items=target_tech_names,
         major_axis=horizons, 
         minor_axis=training_years)
technology_models_prediction['improvement~N(mu,sigma)'] = pd.Panel(items=target_tech_names,
         major_axis=horizons, 
         minor_axis=training_years)

first_year = data.index[0]
for tech_name in target_tech_names:
    print('------------------------------------')
    print(tech_name)
    print('------------------------------------')
    
    time_series = log10(data[tech_name]**-1)
    
    for training_year in training_years:
        print(training_year)
        
        start_ind = int(training_year-first_year)
        
        if sum(~isnan(time_series.values[:start_ind]))>2:            
            stan_data = {'N_time_periods': len(time_series),
                    'N_time_periods_for_inference': start_ind,
                     'Y': pd.Series(time_series).fillna(-999),
                    'mu_prior_location': .1,
                    'mu_prior_scale': 2,
                    'sigma_prior_location': 1,
                    'sigma_prior_scale': 2}

            ###
            model_fit = stanity.fit(model_code['improvement~N(mu,sigma)'], data=stan_data, n_jobs=n_jobs)
            print(_print_stanfit(model_fit, ['mu', 'sigma']))

            for horizon in horizons:
                if horizon=='all':
                    ll = nanmean(model_fit['log_likelihood'][:,start_ind:])
                else:
                    ll = nanmean(model_fit['log_likelihood'][:,start_ind:start_ind+horizon])
                technology_models_prediction['improvement~N(mu,sigma)'].ix[tech_name,horizon,training_year] = ll

            ###
            stan_data['theta_prior_location'] = .5
            stan_data['theta_prior_scale'] = 2
            
            model_fit = stanity.fit(model_code['improvement~p(theta)N(mu,sigma)'], data=stan_data, n_jobs=n_jobs)
            print(_print_stanfit(model_fit, ['mu', 'sigma', 'theta']))

            for horizon in horizons:
                if horizon=='all':
                    ll = nanmean(model_fit['log_likelihood'][:,start_ind:])
                else:
                    ll = nanmean(model_fit['log_likelihood'][:,start_ind:start_ind+horizon])
                technology_models_prediction['improvement~p(theta)N(mu,sigma)'].ix[tech_name,horizon,training_year] = ll

technology_models_prediction['improvement~p(theta)N(mu,sigma)']

technology_models_prediction['improvement~p(theta)N(mu,sigma)'].notnull().mean(axis=0)

technology_models_prediction['improvement~p(theta)N(mu,sigma)'].mean(axis=0)

technology_models_prediction['improvement~p(theta)N(mu,sigma)'].mean(axis=0).T.plot()
yscale('symlog')

technology_models_prediction['improvement~N(mu,sigma)'].mean(axis=0).T.plot()
yscale('symlog')

for i in arange(len(horizons)):
    figure()
    technology_models_prediction['improvement~p(theta)N(mu,sigma)'].mean(axis=0).iloc[i].plot(label='p(theta)N(mu,sigma)')
    technology_models_prediction['improvement~N(mu,sigma)'].mean(axis=0).iloc[i].plot(ax=gca(),label='N(mu,sigma)')
    yscale('symlog')
    title(str(horizons[i])+' year forecast horizon')
    xlabel("Training Model with data up to this year")
    ylabel("log(likelihood) of subsequent data within the time horizon")

model_code['improvement~N(mu,sigma), multiple'] = """
functions{
    int first_observation_ind(vector my_array){
        int t;
        t = 1;
        while(my_array[t] < -900){
          t = t+1;
        }
        return t;
    }
    
    int last_observation_ind(vector my_array, int length){
        int last_observation;
        last_observation = 0; 
        for(t in 1:length){
          if(my_array[t] > -900){
              last_observation = t;
          }
        }
        return last_observation;
    }
    
    
    int count_n_observations(vector my_array) {
        int count;
        count = 0;
        for (t in 1:num_elements(my_array)) {
            if(my_array[t] > -900){
                count = count + 1;
            }
        }
        return count;
    }
    
    real lower_truncated_normal_lpdf(real x, real mu, real sigma, real A) {
        return(normal_lpdf(x | mu, sigma) - normal_lccdf(A | mu, sigma));
    }
}

data {
    int N_technologies;
    int N_time_periods; // number of time periods
    vector[N_time_periods] Y[N_technologies]; // value each time period
    
    int N_time_periods_for_inference;
    
    real mu_prior_location;
    real mu_prior_scale;
    
    real sigma_prior_location;
    real sigma_prior_scale;
 
}

transformed data {
  int first_observation[N_technologies];
  int last_observation[N_technologies];
  int N_observed_time_periods_for_inference[N_technologies];
  int r_observation_offset[N_technologies];
  int n_observations[N_technologies];
  int r_array_offset[N_technologies];

  for (tech in 1:N_technologies){
      first_observation[tech] = first_observation_ind(Y[tech][1:N_time_periods_for_inference]);
      last_observation[tech] = last_observation_ind(Y[tech][1:N_time_periods_for_inference], 
                          N_time_periods_for_inference);

      N_observed_time_periods_for_inference[tech] = last_observation[tech]-first_observation[tech] + 1;
      r_observation_offset[tech] = first_observation[tech]-1;
      n_observations[tech] = count_n_observations(Y[tech]);
  }
  r_array_offset[1] = 0;
  for (tech in 2:N_technologies){
    r_array_offset[tech] = N_observed_time_periods_for_inference[tech-1]+r_array_offset[tech-1]-1;
  }
}
  
parameters {
    real<lower = 0> mu[N_technologies];
    real<lower = 0> sigma[N_technologies];

    vector<lower = 0,upper = 1>[sum(N_observed_time_periods_for_inference)-
                                N_technologies] r_raw; // updates
}

transformed parameters {
  // Identify where the first and last non-missing data points are in Y
  vector<lower = 0>[sum(N_observed_time_periods_for_inference)-
                                N_technologies] r; // updates
  
  {
  // Dictate that the total change between each pair of observations is equal to the observed change between them
  // This is relevant for time periods with missing data
  int most_recent_observation;
  for (tech in 1:N_technologies){
      most_recent_observation = first_observation[tech];
      for(t in first_observation[tech]+1:last_observation[tech]) {
          if(Y[tech][t] > -900) {
            r[(r_array_offset[tech]+most_recent_observation-r_observation_offset[tech]):
              (r_array_offset[tech]+(t-1)-r_observation_offset[tech])] = 
            r_raw[(r_array_offset[tech]+most_recent_observation-r_observation_offset[tech]):
              (r_array_offset[tech]+(t-1)-r_observation_offset[tech])] /
            sum(r_raw[(r_array_offset[tech]+most_recent_observation-r_observation_offset[tech]):
              (r_array_offset[tech]+(t-1)-r_observation_offset[tech])]) * 
            (Y[tech][t]-Y[tech][most_recent_observation]);
            most_recent_observation = t;
            }
        }
      }
  }
}

model {
    mu ~ normal(mu_prior_location, mu_prior_scale);
    sigma ~ cauchy(sigma_prior_location, sigma_prior_scale);

    for (tech in 1:N_technologies){
        for(t in 1:N_observed_time_periods_for_inference[tech]-1){
            target += lower_truncated_normal_lpdf(r[r_array_offset[tech]+t] | mu[tech], sigma[tech], 0);
        }
    }
}

generated quantities {
    vector[N_time_periods] Y_sim[N_technologies];
    vector[N_time_periods] log_likelihood[N_technologies];
    //real mean_change[N_technologies];
    //real variance_change[N_technologies];
    
    //for (tech in 1:N_technologies){
    //    mean_change = mean(r[r_array_offset[tech]:(r_array_offset[tech]+N_observed_time_periods_for_inference[tech])]);
    //    variance_change = variance(r[r_array_offset[tech]:(r_array_offset[tech]+N_observed_time_periods_for_inference[tech])]);
    //}
    
    //Fill out data in the missing periods
    for (tech in 1:N_technologies){
        for(t in first_observation[tech]:last_observation[tech]) {
          if(Y[tech][t] > -900){
              Y_sim[tech][t] = Y[tech][t];
          } else{
              Y_sim[tech][t] = Y_sim[tech][t-1] + r[r_array_offset[tech]+(t-1)-r_observation_offset[tech]];
          } 
        }
    }
    {
    real increase_size;
    //Fill out future data points
    for (tech in 1:N_technologies){
        for(t in last_observation[tech]+1:N_time_periods){
            // Stan cannot yet generate numbers from a truncated distribution directly, so we have to do this silly thing. 
            // As of version 2.12.0, the devs are still talking about it: https://github.com/stan-dev/math/issues/214
            increase_size = -1.0;  
            while (increase_size<0){
                increase_size = normal_rng(mu[tech],sigma[tech]);
            }
            Y_sim[tech][t] = increase_size + Y_sim[tech][t-1];
        }
    }
    }
    
    //Fill out past data points
    {
    int t;
    real increase_size;
    for (tech in 1:N_technologies){
        t = first_observation[tech];
        while(t>1){
            increase_size = -1.0;  
            while (increase_size<0){
                increase_size = normal_rng(mu[tech],sigma[tech]);
            }
            Y_sim[tech][t-1] = Y_sim[tech][t] - increase_size;
            t = t-1;
        }
    }
    }

    for (tech in 1:N_technologies){
        for(t in 2:N_time_periods){
            if(Y[tech][t] > -900){
                    log_likelihood[tech][t] = lower_truncated_normal_lpdf(Y[tech][t]-
                                                                        Y_sim[tech][t-1]| mu[tech], sigma[tech], 0);
            }
        }
    }
}
"""

model_code['improvement~N(mu,sigma), hierarchical'] = """
functions{
    int first_observation_ind(vector my_array){
        int t;
        t = 1;
        while(my_array[t] < -900){
          t = t+1;
        }
        return t;
    }
    
    int last_observation_ind(vector my_array, int length){
        int last_observation;
        last_observation = 0; 
        for(t in 1:length){
          if(my_array[t] > -900){
              last_observation = t;
          }
        }
        return last_observation;
    }
    
    
    int count_n_observations(vector my_array) {
        int count;
        count = 0;
        for (t in 1:num_elements(my_array)) {
            if(my_array[t] > -900){
                count = count + 1;
            }
        }
        return count;
    }
    
    real lower_truncated_normal_lpdf(real x, real mu, real sigma, real A) {
        return(normal_lpdf(x | mu, sigma) - normal_lccdf(A | mu, sigma));
    }
}

data {
    int N_technologies;
    int N_time_periods; // number of time periods
    vector[N_time_periods] Y[N_technologies]; // value each time period
    
    int N_time_periods_for_inference;
    
    real mu_prior_location;
    real mu_prior_scale;
    
    real sigma_prior_location;
    real sigma_prior_scale;
 
}

transformed data {
  int first_observation[N_technologies];
  int last_observation[N_technologies];
  int N_observed_time_periods_for_inference[N_technologies];
  int r_observation_offset[N_technologies];
  int n_observations[N_technologies];
  int r_array_offset[N_technologies];

  for (tech in 1:N_technologies){
      first_observation[tech] = first_observation_ind(Y[tech][1:N_time_periods_for_inference]);
      last_observation[tech] = last_observation_ind(Y[tech][1:N_time_periods_for_inference], 
                          N_time_periods_for_inference);

      N_observed_time_periods_for_inference[tech] = last_observation[tech]-first_observation[tech] + 1;
      r_observation_offset[tech] = first_observation[tech]-1;
      n_observations[tech] = count_n_observations(Y[tech]);
  }
  r_array_offset[1] = 0;
  for (tech in 2:N_technologies){
    r_array_offset[tech] = N_observed_time_periods_for_inference[tech-1]+r_array_offset[tech-1]-1;
  }
}
  
parameters {
    vector<lower = 0>[N_technologies] mu;
    vector<lower = 0>[N_technologies] sigma;

    vector<lower = 0,upper = 1>[sum(N_observed_time_periods_for_inference)-
                                N_technologies] r_raw; // updates
                                
    corr_matrix[2] Omega;
    vector<lower = 0>[2] tau;
    vector<lower=0>[2] mu_parvec;
}

transformed parameters {
  // Identify where the first and last non-missing data points are in Y
  vector<lower = 0>[sum(N_observed_time_periods_for_inference)-
                                N_technologies] r; // updates
  matrix[N_technologies, 2] parvec;
  parvec = append_col(mu, sigma);
  
  {
  // Dictate that the total change between each pair of observations is equal to the observed change between them
  // This is relevant for time periods with missing data
  int most_recent_observation;
  for (tech in 1:N_technologies){
      most_recent_observation = first_observation[tech];
      for(t in first_observation[tech]+1:last_observation[tech]) {
          if(Y[tech][t] > -900) {
            r[(r_array_offset[tech]+most_recent_observation-r_observation_offset[tech]):
              (r_array_offset[tech]+(t-1)-r_observation_offset[tech])] = 
            r_raw[(r_array_offset[tech]+most_recent_observation-r_observation_offset[tech]):
              (r_array_offset[tech]+(t-1)-r_observation_offset[tech])] /
            sum(r_raw[(r_array_offset[tech]+most_recent_observation-r_observation_offset[tech]):
              (r_array_offset[tech]+(t-1)-r_observation_offset[tech])]) * 
            (Y[tech][t]-Y[tech][most_recent_observation]);
            most_recent_observation = t;
            }
        }
      }
  }  
}

model {
    tau ~ cauchy(0, 1);
    Omega ~ lkj_corr(4);
    mu_parvec[1] ~ student_t(3, 1, 1);
    mu_parvec[2] ~ student_t(3, 0.5, 0.5);
    
    for (tech in 1:N_technologies){
        parvec[tech] ~ multi_normal(mu_parvec, quad_form_diag(Omega, tau));
        for(t in 1:N_observed_time_periods_for_inference[tech]-1){
            target += lower_truncated_normal_lpdf(r[r_array_offset[tech]+t] | mu[tech], sigma[tech], 0);
        }
    }
}

generated quantities {
    vector[N_time_periods] Y_sim[N_technologies];
    vector[N_time_periods] log_likelihood[N_technologies];
    //real mean_change[N_technologies];
    //real variance_change[N_technologies];
    
    //for (tech in 1:N_technologies){
    //    mean_change = mean(r[r_array_offset[tech]:(r_array_offset[tech]+N_observed_time_periods_for_inference[tech])]);
    //    variance_change = variance(r[r_array_offset[tech]:(r_array_offset[tech]+N_observed_time_periods_for_inference[tech])]);
    //}
    
    //Fill out data in the missing periods
    for (tech in 1:N_technologies){
        for(t in first_observation[tech]:last_observation[tech]) {
          if(Y[tech][t] > -900){
              Y_sim[tech][t] = Y[tech][t];
          } else{
              Y_sim[tech][t] = Y_sim[tech][t-1] + r[r_array_offset[tech]+(t-1)-r_observation_offset[tech]];
          } 
        }
    }
    {
    real increase_size;
    //Fill out future data points
    for (tech in 1:N_technologies){
        for(t in last_observation[tech]+1:N_time_periods){
            // Stan cannot yet generate numbers from a truncated distribution directly, so we have to do this silly thing. 
            // As of version 2.12.0, the devs are still talking about it: https://github.com/stan-dev/math/issues/214
            increase_size = -1.0;  
            while (increase_size<0){
                increase_size = normal_rng(mu[tech],sigma[tech]);
            }
            Y_sim[tech][t] = increase_size + Y_sim[tech][t-1];
        }
    }
    }
    
    //Fill out past data points
    {
    int t;
    real increase_size;
    for (tech in 1:N_technologies){
        t = first_observation[tech];
        while(t>1){
            increase_size = -1.0;  
            while (increase_size<0){
                increase_size = normal_rng(mu[tech],sigma[tech]);
            }
            Y_sim[tech][t-1] = Y_sim[tech][t] - increase_size;
            t = t-1;
        }
    }
    }

    for (tech in 1:N_technologies){
        for(t in 2:N_time_periods){
            if(Y[tech][t] > -900){
                    log_likelihood[tech][t] = lower_truncated_normal_lpdf(Y[tech][t]-
                                                                        Y_sim[tech][t-1]| mu[tech], sigma[tech], 0);
            }
        }
    }
}
"""

### Random walk, missing data, positive steps, multiple technologies
p_missing = 0.3
mu = 5
sigma = 1
n = 100


a = -mu / sigma
data = truncnorm(a, inf, loc=mu,scale=sigma).rvs(n)

time_series = cumsum(data)
missing = rand(n)<p_missing
missing[0] = False
missing[-1] = False
time_series[missing]=nan

time_series0 = time_series


p_missing = 0.2
mu = 3
sigma = 2
n = 100


a = -mu / sigma
data = truncnorm(a, inf, loc=mu,scale=sigma).rvs(n)

time_series = cumsum(data)
missing = rand(n)<p_missing
missing[0] = False
missing[-1] = False
time_series[missing]=nan

time_series1 = time_series


stan_data = {'N_technologies': 2,
            'N_time_periods': n,
            'N_time_periods_for_inference': n,
             'Y': pd.DataFrame([time_series0, time_series1]).fillna(-999).values,
            'mu_prior_location': 3,
            'mu_prior_scale': 1,
            'sigma_prior_location': 1,
            'sigma_prior_scale': 2}

model_fit = stanity.fit(model_code['improvement~N(mu,sigma), multiple'], data=stan_data, n_jobs=n_jobs)

print(_print_stanfit(model_fit, ['mu', 'sigma']))

### Random walk, missing data, positive steps, multiple technologies, hierarchical
p_missing = 0.3
mu = 5
sigma = 1
n = 100


a = -mu / sigma
data = truncnorm(a, inf, loc=mu,scale=sigma).rvs(n)

time_series = cumsum(data)
missing = rand(n)<p_missing
missing[0] = False
missing[-1] = False
time_series[missing]=nan

time_series0 = time_series


p_missing = 0.2
mu = 3
sigma = 2
n = 100


a = -mu / sigma
data = truncnorm(a, inf, loc=mu,scale=sigma).rvs(n)

time_series = cumsum(data)
missing = rand(n)<p_missing
missing[0] = False
missing[-1] = False
time_series[missing]=nan

time_series1 = time_series


stan_data = {'N_technologies': 2,
            'N_time_periods': n,
            'N_time_periods_for_inference': n,
             'Y': pd.DataFrame([time_series0, time_series1]).fillna(-999).values,
            'mu_prior_location': 3,
            'mu_prior_scale': 1,
            'sigma_prior_location': 1,
            'sigma_prior_scale': 2}

model_fit = stanity.fit(model_code['improvement~N(mu,sigma), hierarchical'], data=stan_data, n_jobs=n_jobs)

print(_print_stanfit(model_fit, ['mu', 'sigma', 'Omega']))

data = pd.read_csv(data_directory+'time_series.csv',index_col=0)
data = data.reindex(arange(data.index[0],data.index[-1]+1))
metadata = pd.read_csv(data_directory+'time_series_metadata.csv')
technology_models_prediction = {}

target_tech_names = metadata.ix[metadata['Type']=='Performance', 'Name']
print("%i technologies"%target_tech_names.shape[0])

training_years = arange(1950,2000,10)
horizons = [5,10,'all']

technology_models_prediction['improvement~N(mu,sigma), multiple'] = pd.Panel(items=target_tech_names,
         major_axis=horizons+['mu', 'sigma'], 
         minor_axis=training_years)

first_year = data.index[0]

time_series = data[target_tech_names]
time_series = log10(time_series**-1)

for training_year in training_years:
    print(training_year)

    start_ind = int(training_year-first_year)
    time_series_from_time_period = time_series.columns[time_series.iloc[:start_ind].notnull().sum(axis=0)>2]
    n_time_series_from_time_period = len(time_series_from_time_period)
    stan_data = {'N_technologies': n_time_series_from_time_period,
                 'N_time_periods': time_series.shape[0],
            'N_time_periods_for_inference': start_ind,
             'Y': time_series[time_series_from_time_period].fillna(-999).T,
                 'mu_prior_location': .1,
                'mu_prior_scale': 2,
                'sigma_prior_location': 1,
                'sigma_prior_scale': 2}

    ###
    model_fit = stanity.fit(model_code['improvement~N(mu,sigma), multiple'], data=stan_data, n_jobs=n_jobs)
    print(_print_stanfit(model_fit, ['mu', 'sigma']))
    technology_models_prediction['improvement~N(mu,sigma), multiple'].ix[time_series_from_time_period,
                                                                             'mu',training_year] = model_fit['mu'].mean(axis=0)
    technology_models_prediction['improvement~N(mu,sigma), multiple'].ix[time_series_from_time_period,
                                                                             'mu',training_year] = model_fit['sigma'].mean(axis=0)
    
    for horizon in horizons:
        if horizon=='all':
            ll = nanmean(nanmean(model_fit['log_likelihood'][:,:,start_ind:],axis=0),axis=1)
        else:
            ll = nanmean(nanmean(model_fit['log_likelihood'][:,:,start_ind:start_ind+horizon],axis=0),axis=1)
        technology_models_prediction['improvement~N(mu,sigma), multiple'].ix[time_series_from_time_period,
                                                                             horizon,training_year] = ll

target_tech_names = metadata.ix[metadata['Type']=='Performance', 'Name']
print("%i technologies"%target_tech_names.shape[0])

training_years = arange(1950,2000,10)
horizons = [5,10,'all']

technology_models_prediction['improvement~N(mu,sigma), hierarchical'] = pd.Panel(items=target_tech_names,
         major_axis=horizons, 
         minor_axis=training_years)

first_year = data.index[0]

time_series = data[target_tech_names]
time_series = log10(time_series**-1)

for training_year in training_years:
    print(training_year)

    start_ind = int(training_year-first_year)
    time_series_from_time_period = time_series.columns[time_series.iloc[:start_ind].notnull().sum(axis=0)>2]
    n_time_series_from_time_period = len(time_series_from_time_period)
    stan_data = {'N_technologies': n_time_series_from_time_period,
                 'N_time_periods': time_series.shape[0],
            'N_time_periods_for_inference': start_ind,
             'Y': time_series[time_series_from_time_period].fillna(-999).T,
                     'mu_prior_location': .1,
                    'mu_prior_scale': 2,
                    'sigma_prior_location': 1,
                    'sigma_prior_scale': 2}

    ###
    model_fit = stanity.fit(model_code['improvement~N(mu,sigma), hierarchical'], data=stan_data, n_jobs=n_jobs)
    print(_print_stanfit(model_fit, ['mu', 'sigma']))

    for horizon in horizons:
        if horizon=='all':
            ll = nanmean(nanmean(model_fit['log_likelihood'][:,:,start_ind:],axis=0),axis=1)
        else:
            ll = nanmean(nanmean(model_fit['log_likelihood'][:,:,start_ind:start_ind+horizon],axis=0),axis=1)
        technology_models_prediction['improvement~N(mu,sigma), hierarchical'].ix[time_series_from_time_period,
                                                                             horizon,training_year] = ll

for i in arange(len(horizons)):
    figure()
#     technology_models_prediction['improvement~N(mu,sigma), multiple'].mean(axis=0).iloc[i].plot(ax=gca(),label='N(mu,sigma)')
    technology_models_prediction['improvement~N(mu,sigma), hierarchical'].mean(axis=0).iloc[i].plot(ax=gca(),label='N(mu,sigma), hierarchical')

    yscale('symlog')
    title(str(horizons[i])+' year forecast horizon')
    xlabel("Training Model with data up to this year")
    ylabel("log(likelihood) of subsequent data within the time horizon")

for i in arange(len(time_series_from_time_period))[:10]:
    figure()
    title(time_series_from_time_period[i])
    plot_time_series_inference(model_fit,ind=i, x=data.index.values)
    scatter(time_series.index, time_series[time_series_from_time_period[i]])
