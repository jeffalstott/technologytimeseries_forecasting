{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_year = 1990\n",
    "data_start_year = 1940\n",
    "# data_type = 'Performance'\n",
    "# model_type = 'pooled'\n",
    "# model_type = 'VAR_separate'\n",
    "# target_predictor = 'meanSPNPcited_1year_before'\n",
    "# 'Citations_Backward_N', \n",
    "# 'Citations_Backward_Age_Mean', 'Citations_Backward_Age_STD', \n",
    "# 'meanSPNPcited_1year_before', 'stdSPNPcited_1year_before',\n",
    "# 'N_Patents\n",
    "output_directory = '/home/jeffrey_alstott/technoinnovation/technologytimeseries_forecasting/src/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setup\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Initial setup\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_color_codes()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jeffrey_alstott/technoinnovation/')\n",
    "sys.path.append('/home/jeffrey_alstott/technoinnovation/technologytimeseries_forecasting/src/')\n",
    "from pystan_time_series import TimeSeriesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Stan settings and testing functions\n",
    "n_jobs = 4\n",
    "n_iterations = 500\n",
    "\n",
    "def check_div(fit, parameters):\n",
    "    div = concatenate([s['divergent__'] for s in fit.get_sampler_params(inc_warmup=False)]).astype('bool')\n",
    "\n",
    "    if sum(div==0):\n",
    "        print(\"\\x1b[32m\\\"No divergences\\\"\\x1b[0m\")\n",
    "    else:\n",
    "        from ndtest import ks2d2s\n",
    "        divergences = {}\n",
    "        non_divergences = {}\n",
    "        for parameter in parameters:\n",
    "            divergences[parameter] = fit[parameter][div].squeeze()\n",
    "            non_divergences[parameter] = fit[parameter][~div].squeeze()\n",
    "            if divergences[parameter].ndim>2:\n",
    "                N = divergences[parameter].shape[3]\n",
    "                for n in arange(N):\n",
    "                    divergences[parameter+'.%i'%n] = divergences[parameter][:,:,n]\n",
    "                    non_divergences[parameter+'.%i'%n] = non_divergences[parameter][:,:,n]\n",
    "                del divergences[parameter]\n",
    "                del non_divergences[parameter]\n",
    "\n",
    "            any_unevenly_distributed = False\n",
    "            \n",
    "            for k1 in divergences.keys():\n",
    "                for k2 in divergences.keys():\n",
    "                    if k1==k2:\n",
    "                        continue\n",
    "\n",
    "                    x = divergences[k1].ravel()\n",
    "                    y = divergences[k2].ravel()\n",
    "\n",
    "                    x_non = non_divergences[k1].ravel()\n",
    "                    y_non = non_divergences[k2].ravel()\n",
    "\n",
    "                    p = ks2d2s(x_non, y_non, x, y)\n",
    "                    if p<.05:\n",
    "                        any_unevenly_distributed = True\n",
    "#                         figure()\n",
    "#                         scatter(x_non, y_non,\n",
    "#                            alpha=.1, label='Non-Divergent')\n",
    "#                         scatter(x,y,\n",
    "#                                alpha=1, label='Divergent')\n",
    "#                         xlabel(k1)\n",
    "#                         ylabel(k2)\n",
    "#                         legend()\n",
    "#                         title(\"KS test p=%.2f\"%(p))\n",
    "        if any_unevenly_distributed:\n",
    "            print(\"\\x1b[31m\\\"%.2f divergences, which appear to be non-spurious\\\"\\x1b[0m\"%(div.mean()))\n",
    "        else:\n",
    "            print(\"\\x1b[32m\\\"%.2f divergences, which appear to be spurious\\\"\\x1b[0m\"%(div.mean()))\n",
    "\n",
    "import stan_utility\n",
    "from pystan.misc import _summary\n",
    "def test_model_fit(fit, parameters, max_depth=10):\n",
    "    Rhats = _summary(fit, pars=parameters)['summary'][:,-1]\n",
    "    if all(abs(Rhats-1)<.1):\n",
    "        c = '32'\n",
    "    else:\n",
    "        c = '31'\n",
    "    print(\"\\x1b[%sm\\\"Maximum Rhat of %.2f\\\"\\x1b[0m\"%(c,max(Rhats)))\n",
    "    stan_utility.check_treedepth(fit,max_depth=max_depth)\n",
    "    stan_utility.check_energy(fit)\n",
    "    check_div(fit, parameters)\n",
    "            \n",
    "from time import time\n",
    "\n",
    "def time_series_with_data_for_training_and_testing(Y, training_year, min_observed=3):\n",
    "    time_series_has_data = (Y[:training_year].notnull().sum(axis=0)>=min_observed) & (Y[training_year+1:].notnull().sum(axis=0)>0)\n",
    "    time_series_has_data = time_series_has_data[time_series_has_data].index\n",
    "    return time_series_has_data\n",
    "\n",
    "def test_prediction(Y_pred, Y_testing):\n",
    "    inds = where(~isnan(Y_testing))\n",
    "    inds = zip(*inds)\n",
    "    from scipy.stats import percentileofscore, gaussian_kde\n",
    "    d = [i+(gaussian_kde(Y_pred[i]).logpdf(Y_testing[i])[0],\n",
    "            percentileofscore(Y_pred[i], Y_testing[i])) for i in inds]\n",
    "    \n",
    "    predictions_df = pd.DataFrame(columns=['K', 'T', 'D', 'lpdf', 'percentile'],\n",
    "                                  data = d\n",
    "                                 )\n",
    "    return predictions_df\n",
    "\n",
    "def test_forecasts(Y_prediction, Y_testing):\n",
    "    forecast_quality = test_prediction(Y_prediction.transpose([1,2,3,0]), Y_testing.values)\n",
    "    for (label, dimension) in [('K', 0), ('T', 1), ('D', 2)]:\n",
    "        forecast_quality[label].replace(arange(len(Y_testing.axes[dimension])), \n",
    "                                                      Y_testing.axes[dimension],\n",
    "                                                     inplace=True)\n",
    "    forecast_quality.set_index(['T', 'K', 'D'], inplace=True)\n",
    "    forecast_quality.sort_index(inplace=True)\n",
    "    return forecast_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Data\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid domains for testing with patent data: 22\n"
     ]
    }
   ],
   "source": [
    "data_directory = '/home/jeffrey_alstott/technoinnovation/technologytimeseries_forecasting/data/'\n",
    "\n",
    "empirical_time_series = pd.read_csv(data_directory+'time_series.csv',index_col=0)\n",
    "empirical_time_series.sort_index(axis=1, inplace=True)\n",
    "empirical_time_series = empirical_time_series.reindex(arange(empirical_time_series.index[0],empirical_time_series.index[-1]+1))\n",
    "metadata = pd.read_csv(data_directory+'time_series_metadata.csv')\n",
    "\n",
    "target_tech_names = metadata.loc[(metadata['Domain'].notnull()), 'Name']\n",
    "time_series_with_domains = empirical_time_series[target_tech_names]\n",
    "\n",
    "valid_time_series = sum(~time_series_with_domains.loc[1976:].isnull())>3\n",
    "valid_domains = metadata.set_index('Name').loc[valid_time_series.index[valid_time_series]]['Domain'].unique()\n",
    "\n",
    "print(\"Number of valid domains for testing with patent data: %i\"%valid_domains.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patent Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patent_data_directory = '/home/jeffrey_alstott/technoinnovation/patent_centralities/data/'\n",
    "\n",
    "patents = pd.read_hdf(patent_data_directory+'patents.h5', 'df')\n",
    "citations = pd.read_hdf(patent_data_directory+'citations.h5', 'df')\n",
    "\n",
    "citations['Citation_Lag'] = citations['Year_Citing_Patent']-citations['Year_Cited_Patent']\n",
    "backward_citations = citations.groupby('Citing_Patent')\n",
    "\n",
    "patents['Citations_Backward_N'] = citations.groupby('Citing_Patent').size()[patents['patent_number']].values\n",
    "patents['Citations_Backward_Age_Mean'] = citations.groupby('Citing_Patent')['Citation_Lag'].mean()[patents['patent_number']].values\n",
    "patents['Citations_Backward_Age_STD'] = citations.groupby('Citing_Patent')['Citation_Lag'].std()[patents['patent_number']].values\n",
    "\n",
    "patent_centralities_z = pd.read_hdf(patent_data_directory+'centralities/summary_statistics.h5', 'empirical_z_scores_USPC')\n",
    "patent_centralities_z.drop('filing_year', axis=1, inplace=True)\n",
    "patents = patents.merge(patent_centralities_z, on='patent_number')\n",
    "\n",
    "patents_percentile_by_year = patents.copy()\n",
    "for col in patents.columns:\n",
    "    if col in ['filing_year', 'patent_number', 'Class']:\n",
    "        continue\n",
    "    patents_percentile_by_year[col] = patents.groupby('filing_year')[col].rank(pct=True)\n",
    "\n",
    "patents_percentile_by_year.set_index('patent_number', inplace=True)\n",
    "\n",
    "n_patents_by_year = patents.groupby('filing_year').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patent_domains = pd.read_csv(data_directory+'PATENT_SET_DOMAINS.csv', index_col=0)\n",
    "def floatify(x):\n",
    "    from numpy import nan\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return nan\n",
    "patent_domains['patent_id'] = patent_domains['patent_id'].apply(floatify)\n",
    "patent_domains = patent_domains.dropna()\n",
    "domains = patent_domains['Domain'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate_predictors = ['Citations_Backward_N',\n",
    "                        'Citations_Backward_Age_Mean',\n",
    "                        'Citations_Backward_Age_STD',\n",
    "                        'meanSPNPcited_1year_before',\n",
    "                        'stdSPNPcited_1year_before',\n",
    "                       ]\n",
    "\n",
    "first_year_for_predictors = 1976\n",
    "for col in candidate_predictors+['filing_year']:\n",
    "    patent_domains[col] = patents_percentile_by_year.loc[patent_domains['patent_id'], col].values\n",
    "predictors_by_domain = patent_domains.groupby(['Domain', 'filing_year'])[candidate_predictors].mean()\n",
    "predictors_by_domain['N_Patents'] = patent_domains.groupby(['Domain', 'filing_year']).size()\n",
    "\n",
    "predictors_by_domain = predictors_by_domain.reindex([(d,y) for d in domains for y in arange(first_year_for_predictors, \n",
    "                                                                                                  patent_domains['filing_year'].max()+1)])\n",
    "\n",
    "predictors_by_domain['N_Patents'] = predictors_by_domain['N_Patents'].fillna(0).values/n_patents_by_year.loc[predictors_by_domain.reset_index()['filing_year']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations_Backward_N\n",
      "Citations_Backward_Age_Mean\n",
      "Citations_Backward_Age_STD\n",
      "meanSPNPcited_1year_before\n",
      "stdSPNPcited_1year_before\n",
      "N_Patents\n"
     ]
    }
   ],
   "source": [
    "patent_time_series = pd.Panel(items=empirical_time_series.columns,\n",
    "                      major_axis=empirical_time_series.index,\n",
    "                      minor_axis=predictors_by_domain.columns,#+['Performance'],\n",
    "                     )\n",
    "\n",
    "for predictor in predictors_by_domain.columns:\n",
    "    print(predictor)\n",
    "    for col in empirical_time_series.columns:\n",
    "        if metadata.set_index('Name').notnull().loc[col, 'Domain']:\n",
    "            patent_time_series.loc[col,:, predictor] = predictors_by_domain.loc[metadata.set_index('Name').loc[col, \n",
    "                                                                                                   'Domain']][predictor]\n",
    "# predictors.fillna(0, inplace=True)\n",
    "# Y_var.loc[:,:, 'Performance'] = Y\n",
    "\n",
    "# combined_df = Y.stack(dropna=False).swaplevel()\n",
    "# combined_df.name = 'Performance'\n",
    "# combined_df = pd.DataFrame(combined_df)\n",
    "\n",
    "has_patent_data = patent_time_series.notnull().any().iloc[0]\n",
    "has_patent_data = has_patent_data[has_patent_data].index.values\n",
    "# Y_var = Y_var.loc[has_patent_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Predictive Models\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:6: DeprecationWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "/home/jeffrey_alstott/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: DeprecationWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_tech_names = metadata.loc[metadata['Type']==data_type, 'Name']\n",
    "Y = log(empirical_time_series[target_tech_names]).loc[data_start_year:]\n",
    "time_series_has_data = time_series_with_data_for_training_and_testing(Y, training_year)\n",
    "Y = Y[time_series_has_data]\n",
    "Y = pd.Panel({data_type:Y}).transpose([2,1,0])\n",
    "Y_training = Y.copy()\n",
    "Y_testing = Y.copy()\n",
    "Y_training.loc[:,training_year+1:] = nan\n",
    "Y_testing.loc[:, :training_year] = nan\n",
    "\n",
    "# Y_testing = Y_testing[time_series_has_data[:10]]\n",
    "# Y_training = Y_training[time_series_has_data[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_types(time_series, model_types, training_year=1990, use_partial_pooling=True):\n",
    "    models = {}\n",
    "    max_depth = 15\n",
    "    for model_name in model_types.keys():\n",
    "        print(model_name+'\\n============================')\n",
    "        start_time = time()\n",
    "        model = TimeSeriesModel(Y=time_series.values, use_partial_pooling=use_partial_pooling, **model_types[model_name])\n",
    "        model.sampling(n_jobs=n_jobs, iter=n_iterations, control={'max_treedepth':max_depth})\n",
    "        test_model_fit(model.fit, ['mu', 'sigma'], max_depth=max_depth)\n",
    "        models[model_name] = model.fit['Y_latent']\n",
    "        print(\"Fitting took %.2f minutes\"%((time()-start_time)/60))\n",
    "    return models\n",
    "\n",
    "def train_model_types_separate(time_series, model_types, training_year=1990):\n",
    "    models = {}\n",
    "    \n",
    "    for model_name in model_types.keys():\n",
    "        print(model_name+'\\n============================')\n",
    "        start_time = time()\n",
    "        separate_models_forecast = []\n",
    "        for c in time_series.items:\n",
    "            print(c)\n",
    "            model = TimeSeriesModel(Y=expand_dims(time_series[c],0), **model_types[model_name])\n",
    "            max_depth = 15\n",
    "            model.sampling(n_jobs=n_jobs, iter=n_iterations, control={'max_treedepth':max_depth})\n",
    "            test_model_fit(model.fit, ['mu', 'sigma'], max_depth=max_depth)\n",
    "            separate_models_forecast.append(model.fit['Y_latent'])\n",
    "            print(\"Fitting has taken %.2f minutes\"%((time()-start_time)/60))\n",
    "        separate_models_forecast = concatenate(separate_models_forecast, axis=1)\n",
    "        models[model_name] = separate_models_forecast\n",
    "        print(\"Fitting took %.2f minutes\"%((time()-start_time)/60))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if data_type=='Performance':\n",
    "    monotonic = [1]\n",
    "else:\n",
    "    monotonic = None\n",
    "\n",
    "model_types = {\n",
    "                'ARIMA(0,0)': {'p':0,'q':0, 'difference': [1], 'monotonic': monotonic},\n",
    "               'ARIMA(0,1)': {'p':0, 'q':1, 'difference': [1], 'monotonic': monotonic},\n",
    "               'ARIMA(1,0)': {'p':1,'q':0, 'difference': [1], 'monotonic': monotonic},\n",
    "               'ARIMA(1,1)': {'p':1,'q':1, 'difference': [1], 'monotonic': monotonic},\n",
    "               'ARIMA([1,5],0)': {'p':[1,5],'q':0, 'difference': [1], 'monotonic': monotonic},\n",
    "               'ARIMA([1,5],1)': {'p':[1,5],'q':1, 'difference': [1], 'monotonic': monotonic},\n",
    "               'ARIMA([1,5],[1,5])': {'p':[1,5],'q':[1,5], 'difference': [1], 'monotonic': monotonic},\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_type=='separate':\n",
    "    print(\"Modeling separate\")\n",
    "# try:\n",
    "#     separate_models_forecasts = pickle.load(open('separate_models_forecasts_%s'%data_type, 'rb'))\n",
    "# except FileNotFoundError:\n",
    "    separate_models_forecasts = train_model_types_separate(Y_training, model_types)\n",
    "    separate_models_forecasts['training'] = Y_training\n",
    "    separate_models_forecasts['testing'] = Y_testing\n",
    "    pickle.dump(separate_models_forecasts, open(output_directory+'separate_models_forecasts_%s'%data_type, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_type=='pooled':\n",
    "    print(\"Modeling pooled\")\n",
    "# try:\n",
    "#     pooled_models_forecasts = pickle.load(open('pooled_models_forecasts_%s'%data_type, 'rb'))\n",
    "# except FileNotFoundError:\n",
    "    pooled_models_forecasts = train_model_types(Y_training, model_types)\n",
    "    pooled_models_forecasts['training'] = Y_training\n",
    "    pooled_models_forecasts['testing'] = Y_testing\n",
    "    pickle.dump(pooled_models_forecasts, open(output_directory+'pooled_models_forecasts_%s'%data_type, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAR model\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_start_year = 1975\n",
    "Y = log(empirical_time_series[target_tech_names]).loc[data_start_year:]\n",
    "time_series_has_data = time_series_with_data_for_training_and_testing(Y, training_year)\n",
    "Y = Y[time_series_has_data]\n",
    "Y = pd.Panel({data_type:Y}).transpose([2,1,0])\n",
    "\n",
    "from scipy.special import logit\n",
    "Y_var = patent_time_series.loc[time_series_has_data, data_start_year:].copy()\n",
    "Y_var.loc[:,:,:] = logit(Y_var).values\n",
    "has_patent_data = Y_var.notnull().any().iloc[0]\n",
    "has_patent_data = has_patent_data[has_patent_data].index.values\n",
    "Y_var.loc[:,:, data_type] = Y.loc[:,:,data_type]\n",
    "Y_var = Y_var.loc[has_patent_data]\n",
    "\n",
    "Y_var = Y_var.loc[:,:,[target_predictor, data_type]]\n",
    "\n",
    "\n",
    "Y_var_training = Y_var.copy()\n",
    "Y_var_testing = Y_var.copy()\n",
    "Y_var_training.loc[:,training_year+1:] = nan\n",
    "Y_var_testing.loc[:,:training_year] = nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if data_type=='Performance':\n",
    "    monotonic = [Y_var_training.shape[-1]-1]\n",
    "else:\n",
    "    monotonic = None\n",
    "model_types = {\n",
    "#                'ARIMA(0,0)': {'p':0,'q':0, 'difference': ones(Y_var.shape[-1]).astype('int')},\n",
    "               'ARIMA(1,0)': {'p':1,'q':0, 'difference': ones(Y_var_training.shape[-1]).astype('int'), 'monotonic':monotonic},\n",
    "               'ARIMA(1,1)': {'p':1,'q':1, 'difference': ones(Y_var.shape[-1]).astype('int'), 'monotonic':monotonic},\n",
    "               'ARIMA([1,5],0)': {'p':[1,5],'q':0, 'difference': ones(Y_var.shape[-1]).astype('int'), 'monotonic':monotonic},\n",
    "               'ARIMA([1,5],1)': {'p':[1,5],'q':1, 'difference': ones(Y_var.shape[-1]).astype('int'), 'monotonic':monotonic},\n",
    "               'ARIMA([1,5],[1,5])': {'p':[1,5],'q':[1,5], 'difference': ones(Y_var.shape[-1]).astype('int'), 'monotonic':monotonic},\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling VAR separate\n",
      "ARIMA(1,0)\n",
      "============================\n",
      "integrated_circuit_memory_transistors_per_die\n",
      "\u001b[31m\"24 of 1000 iterations saturated the maximum tree depth of 15 (2.4%)\"\u001b[0m\n",
      "\u001b[31m\"Run again with max_depth set to a larger value to avoid saturation\"\u001b[0m\n",
      "\u001b[32m\"Chain 0: E-BFMI = 0.3905218444107399\"\u001b[0m\n",
      "\u001b[32m\"Chain 1: E-BFMI = 0.2610092703151114\"\u001b[0m\n",
      "\u001b[31m\"Chain 2: E-BFMI = 0.1612669402708438\"\u001b[0m\n",
      "\u001b[31m\"E-BFMI below 0.2 indicates you may need to reparameterize your model\"\u001b[0m\n",
      "\u001b[31m\"Chain 3: E-BFMI = 0.1642536854125112\"\u001b[0m\n",
      "\u001b[31m\"E-BFMI below 0.2 indicates you may need to reparameterize your model\"\u001b[0m\n",
      "\u001b[32m\"No divergences\"\u001b[0m\n",
      "Fitting has taken 7.00 minutes\n",
      "integrated_circuit_microprocessor_transistors_per_die\n",
      "\u001b[31m\"92 of 1000 iterations saturated the maximum tree depth of 15 (9.2%)\"\u001b[0m\n",
      "\u001b[31m\"Run again with max_depth set to a larger value to avoid saturation\"\u001b[0m\n",
      "\u001b[31m\"Chain 0: E-BFMI = 0.18810550026930795\"\u001b[0m\n",
      "\u001b[31m\"E-BFMI below 0.2 indicates you may need to reparameterize your model\"\u001b[0m\n",
      "\u001b[31m\"Chain 1: E-BFMI = 0.17528358403240282\"\u001b[0m\n",
      "\u001b[31m\"E-BFMI below 0.2 indicates you may need to reparameterize your model\"\u001b[0m\n",
      "\u001b[31m\"Chain 2: E-BFMI = 0.14293442727151934\"\u001b[0m\n",
      "\u001b[31m\"E-BFMI below 0.2 indicates you may need to reparameterize your model\"\u001b[0m\n",
      "\u001b[31m\"Chain 3: E-BFMI = 0.15154907542521814\"\u001b[0m\n",
      "\u001b[31m\"E-BFMI below 0.2 indicates you may need to reparameterize your model\"\u001b[0m\n",
      "\u001b[32m\"No divergences\"\u001b[0m\n",
      "Fitting has taken 16.06 minutes\n",
      "02B_magnetic_memory_harddisk_mbits_per_cc\n",
      "\u001b[31m\"9 of 1000 iterations saturated the maximum tree depth of 15 (0.9%)\"\u001b[0m\n",
      "\u001b[31m\"Run again with max_depth set to a larger value to avoid saturation\"\u001b[0m\n",
      "\u001b[32m\"Chain 0: E-BFMI = 0.36716322671377394\"\u001b[0m\n",
      "\u001b[32m\"Chain 1: E-BFMI = 0.24888457171055844\"\u001b[0m\n",
      "\u001b[32m\"Chain 2: E-BFMI = 0.2344330576143026\"\u001b[0m\n",
      "\u001b[31m\"Chain 3: E-BFMI = 0.18608976176890188\"\u001b[0m\n",
      "\u001b[31m\"E-BFMI below 0.2 indicates you may need to reparameterize your model\"\u001b[0m\n",
      "\u001b[32m\"No divergences\"\u001b[0m\n",
      "Fitting has taken 21.90 minutes\n",
      "02C_magnetic_memory_tape_harddisk_mbits_per_cc\n"
     ]
    }
   ],
   "source": [
    "if model_type == 'VAR_separate':\n",
    "    print(\"Modeling VAR separate\")\n",
    "    file_name = output_directory+'VAR_%s_separate_models_forecasts_%s'%(target_predictor, data_type)\n",
    "    VAR_models_forecasts = train_model_types_separate(Y_var_training, model_types)\n",
    "    VAR_models_forecasts['training'] = Y_var_training\n",
    "    VAR_models_forecasts['testing'] = Y_var_testing\n",
    "    pickle.dump(VAR_models_forecasts, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_type == 'VAR_pooled':\n",
    "    print(\"Modeling VAR pooled\")\n",
    "    # try:\n",
    "    #     VAR_models_forecasts = pickle.load(open('VAR_models_forecasts_%s'%data_type, 'rb'))\n",
    "    # except FileNotFoundError:\n",
    "    VAR_models_forecasts = train_model_types(Y_var_training, model_types)\n",
    "    VAR_models_forecasts['training'] = Y_var_training\n",
    "    VAR_models_forecasts['testing'] = Y_var_testing\n",
    "    pickle.dump(VAR_models_forecasts, open('VAR_pooled_models_forecasts_%s'%data_type, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
